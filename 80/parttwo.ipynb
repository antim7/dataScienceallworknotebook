{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c18d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4baccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 SRM filtes\n",
    "from srm_filter_kernel import all_normalized_hpf_list  \n",
    "#Global covariance pooling\n",
    "from MPNCOV import * #MPNCOV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4271159",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_dir = '/home/ahmed/Documents/suniward0.4/base/TRN/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f156090",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32 // 2\n",
    "\n",
    "EPOCHS = 200\n",
    "LR = 0.01\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "TRAIN_PRINT_FREQUENCY = 100\n",
    "EVAL_PRINT_FREQUENCY = 1\n",
    "DECAY_EPOCH = [80, 140, 180]\n",
    "\n",
    "# OUTPUT_PATH = Path(__file__).stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71eb3ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncation operation\n",
    "class TLU(nn.Module):\n",
    "  def __init__(self, threshold):\n",
    "    super(TLU, self).__init__()\n",
    "\n",
    "    self.threshold = threshold\n",
    "\n",
    "  def forward(self, input):\n",
    "    output = torch.clamp(input, min=-self.threshold, max=self.threshold)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a05b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing Module\n",
    "class HPF(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HPF, self).__init__()\n",
    "\n",
    "    #Load 30 SRM Filters\n",
    "    all_hpf_list_5x5 = []\n",
    "\n",
    "    for hpf_item in all_normalized_hpf_list:\n",
    "      if hpf_item.shape[0] == 3:\n",
    "        hpf_item = np.pad(hpf_item, pad_width=((1, 1), (1, 1)), mode='constant')\n",
    "\n",
    "      all_hpf_list_5x5.append(hpf_item)\n",
    "\n",
    "    hpf_weight = nn.Parameter(torch.Tensor(all_hpf_list_5x5).view(30, 1, 5, 5), requires_grad=False)\n",
    "\n",
    "\n",
    "    self.hpf = nn.Conv2d(1, 30, kernel_size=5, padding=2, bias=False)\n",
    "    self.hpf.weight = hpf_weight\n",
    "\n",
    "    #Truncation, threshold = 3 \n",
    "    self.tlu = TLU(3.0)\n",
    "\n",
    "  def forward(self, input):\n",
    "\n",
    "    output = self.hpf(input)\n",
    "    output = self.tlu(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    self.group1 = HPF()\n",
    "\n",
    "    self.group2 = nn.Sequential(\n",
    "      nn.Conv2d(30, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "    )\n",
    "\n",
    "    self.group3 = nn.Sequential(\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "    )\n",
    "\n",
    "    self.group4 = nn.Sequential(\n",
    "      nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n",
    "    )\n",
    "\n",
    "    self.group5 = nn.Sequential(\n",
    "      nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm2d(256),\n",
    "      nn.ReLU(),\n",
    "\n",
    "    )\n",
    "\n",
    "    self.fc1 = nn.Linear(int(256 * (256 + 1) / 2), 2)\n",
    "\n",
    "\n",
    "  def forward(self, input):\n",
    "    output = input\n",
    "\n",
    "    output = self.group1(output)\n",
    "    output = self.group2(output)\n",
    "    output = self.group3(output)\n",
    "    output = self.group4(output)\n",
    "    output = self.group5(output)\n",
    "    \n",
    "    #Global covariance pooling\n",
    "    output = CovpoolLayer(output)\n",
    "    output = SqrtmLayer(output, 5)\n",
    "    output = TriuvecLayer(output)\n",
    "\n",
    "    output = output.view(output.size(0), -1)\n",
    "    output = self.fc1(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.val = 0\n",
    "    self.avg = 0\n",
    "    self.sum = 0\n",
    "    self.count = 0\n",
    "\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "  batch_time = AverageMeter() \n",
    "  data_time = AverageMeter()\n",
    "  losses = AverageMeter()\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  end = time.time()\n",
    "\n",
    "  for i, sample in enumerate(train_loader):\n",
    "\n",
    "    data_time.update(time.time() - end) \n",
    "\n",
    "    data, label = sample['data'], sample['label']\n",
    "\n",
    "    shape = list(data.size())\n",
    "    data = data.reshape(shape[0] * shape[1], *shape[2:])\n",
    "    label = label.reshape(-1)\n",
    "\n",
    "    data, label = data.to(device), label.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    output = model(data)  #FP\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    losses.update(loss.item(), data.size(0))\n",
    "\n",
    "    loss.backward()       #BP\n",
    "    optimizer.step()\n",
    "\n",
    "    batch_time.update(time.time() - end) #BATCH TIME = BATCH BP+FP\n",
    "    end = time.time()\n",
    "\n",
    "    if i % TRAIN_PRINT_FREQUENCY == 0:\n",
    "\n",
    "      logging.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "#Adjust BN estimated value\n",
    "def adjust_bn_stats(model, device, train_loader):\n",
    "  model.train()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for sample in train_loader:\n",
    "      data, label = sample['data'], sample['label']\n",
    "\n",
    "      shape = list(data.size())\n",
    "      data = data.reshape(shape[0] * shape[1], *shape[2:])\n",
    "      label = label.reshape(-1)\n",
    "\n",
    "      data, label = data.to(device), label.to(device)\n",
    "\n",
    "      output = model(data)\n",
    "\n",
    "\n",
    "def evaluate(model, device, eval_loader, epoch, optimizer, best_acc, PARAMS_PATH):\n",
    "  model.eval()\n",
    "\n",
    "  test_loss = 0.0\n",
    "  correct = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for sample in eval_loader:\n",
    "      data, label = sample['data'], sample['label']\n",
    "\n",
    "      shape = list(data.size())\n",
    "      data = data.reshape(shape[0] * shape[1], *shape[2:])\n",
    "      label = label.reshape(-1)\n",
    "\n",
    "      data, label = data.to(device), label.to(device)\n",
    "\n",
    "      output = model(data)\n",
    "      pred = output.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "  accuracy = correct / (len(eval_loader.dataset) * 2)\n",
    "\n",
    "  if accuracy > best_acc and epoch > 140:\n",
    "    best_acc = accuracy\n",
    "    all_state = {\n",
    "      'original_state': model.state_dict(),\n",
    "      'optimizer_state': optimizer.state_dict(),\n",
    "      'epoch': epoch\n",
    "    }\n",
    "    torch.save(all_state, PARAMS_PATH)\n",
    "  \n",
    "  logging.info('-' * 8)\n",
    "  logging.info('Eval accuracy: {:.4f}'.format(accuracy))\n",
    "  logging.info('Best accuracy:{:.4f}'.format(best_acc))   \n",
    "  logging.info('-' * 8)\n",
    "  return best_acc\n",
    "\n",
    "#Initialization\n",
    "def initWeights(module):\n",
    "  if type(module) == nn.Conv2d:\n",
    "    if module.weight.requires_grad:\n",
    "      nn.init.kaiming_normal_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "  if type(module) == nn.Linear:\n",
    "    nn.init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "    nn.init.constant_(module.bias.data, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e88947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data augmentation \n",
    "class AugData():\n",
    "  def __call__(self, sample):\n",
    "    data, label = sample['data'], sample['label']\n",
    "\n",
    "    #Rotation\n",
    "    rot = random.randint(0,3)\n",
    "    data = np.rot90(data, rot, axes=[1, 2]).copy()\n",
    "    \n",
    "    #Mirroring \n",
    "    if random.random() < 0.5:\n",
    "      data = np.flip(data, axis=2).copy()\n",
    "\n",
    "    new_sample = {'data': data, 'label': label}\n",
    "\n",
    "    return new_sample\n",
    "\n",
    "\n",
    "class ToTensor():\n",
    "  def __call__(self, sample):\n",
    "    data, label = sample['data'], sample['label']\n",
    "\n",
    "    data = np.expand_dims(data, axis=1)\n",
    "    data = data.astype(np.float32)\n",
    "    # data = data / 255.0\n",
    "\n",
    "    new_sample = {\n",
    "      'data': torch.from_numpy(data),\n",
    "      'label': torch.from_numpy(label).long(),\n",
    "    }\n",
    "\n",
    "    return new_sample\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, DATASET_DIR, partition, transform=None):\n",
    "        random.seed(1234)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.cover_dir = DATASET_DIR + '/cover'\n",
    "        self.stego_dir = DATASET_DIR + '/stego/' + Model_NAME\n",
    "\n",
    "\n",
    "        self.cover_dir_2 = '/media/castor/Elements/BOSS_off/BOSS_off_100' + '/cover'\n",
    "        self.stego_dir_2 = '/media/castor/Elements/BOSS_off/BOSS_off_100' + '/stego/' + Model_NAME\n",
    "        self.cover_dir_3 = '/media/castor/Elements/BOSS_off/BOSS_off_256' + '/cover'\n",
    "        self.stego_dir_3 = '/media/castor/Elements/BOSS_off/BOSS_off_256' + '/stego/' + Model_NAME\n",
    "        self.cover_dir_4 = '/media/castor/Elements/BOSS_off/BOSS_off_400' + '/cover'\n",
    "        self.stego_dir_4 = '/media/castor/Elements/BOSS_off/BOSS_off_400' + '/stego/' + Model_NAME\n",
    "\n",
    "        self.covers_list_all = [x.split('/')[-1] for x in glob(self.cover_dir + '/*')]\n",
    "        random.shuffle(self.covers_list_all)\n",
    "        if (partition == 0):\n",
    "            self.cover_list = self.covers_list_all[:4000]\n",
    "            self.cover_paths= [os.path.join(self.cover_dir, x) for x in  self.cover_list]\n",
    "            self.cover_paths_2 = [os.path.join(self.cover_dir_2, x) for x in self.cover_list]\n",
    "            self.cover_paths_3 = [os.path.join(self.cover_dir_3, x) for x in self.cover_list]\n",
    "            self.cover_paths_4 = [os.path.join(self.cover_dir_4, x) for x in self.cover_list]\n",
    "            self.cover_paths = self.cover_paths + self.cover_paths_2 + self.cover_paths_3 + self.cover_paths_4#\n",
    "            #print (self.cover_paths_3)\n",
    "            #print self.cover_paths\n",
    "            self.stego_paths = [os.path.join(self.stego_dir, x) for x in self.cover_list]\n",
    "            self.stego_paths_2 = [os.path.join(self.stego_dir_2, x) for x in self.cover_list]\n",
    "            self.stego_paths_3 = [os.path.join(self.stego_dir_3, x) for x in self.cover_list]\n",
    "            self.stego_paths_4 = [os.path.join(self.stego_dir_4, x) for x in self.cover_list]\n",
    "            self.stego_paths = self.stego_paths + self.stego_paths_2 + self.stego_paths_3 + self.stego_paths_4#\n",
    "\n",
    "            self.cover_steg = list(zip(self.cover_paths, self.stego_paths))\n",
    "            random.shuffle(self.cover_steg)\n",
    "            self.cover_paths, self.stego_paths = zip(*self.cover_steg)\n",
    "\n",
    "\n",
    "        if (partition == 1):\n",
    "            self.cover_list = self.covers_list_all[4000:5000]\n",
    "            self.cover_paths = [os.path.join(self.cover_dir, x) for x in self.cover_list]\n",
    "            self.stego_paths = [os.path.join(self.stego_dir, x) for x in self.cover_list]\n",
    "        if (partition == 2):\n",
    "            self.cover_list = self.covers_list_all[5000:10000]\n",
    "            self.cover_paths = [os.path.join(self.cover_dir, x) for x in self.cover_list]\n",
    "            self.stego_paths = [os.path.join(self.stego_dir, x) for x in self.cover_list]\n",
    "\n",
    "        assert len(self.cover_paths) != 0, \"cover_dir is empty\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cover_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_index = int(idx)\n",
    "\n",
    "        cover_path = self.cover_paths[file_index]\n",
    "        stego_path = self.stego_paths[file_index]\n",
    "        cover_data = cv2.imread(cover_path, -1)\n",
    "        stego_data = cv2.imread(stego_path, -1)\n",
    "        data = np.stack([cover_data, stego_data])\n",
    "        label = np.array([0, 1], dtype='int32')\n",
    "\n",
    "        sample = {'data': data, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def setLogger(log_path, mode='a'):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path, mode=mode)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s: %(message)s', '%Y-%m-%d %H:%M:%S'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    statePath = args.statePath\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        #AugData(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    eval_transform = transforms.Compose([\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    TRAIN_DATASET_DIR = args.TRAIN_DIR\n",
    "    VALID_DATASET_DIR = args.VALID_DIR\n",
    "    TEST_DATASET_DIR = args.TEST_DIR\n",
    "\n",
    "    # Log files\n",
    "    global Model_NAME\n",
    "    Model_NAME = 'STEGO_Suniward_P0.4'#'STEGO_Suniward_P0.2'\n",
    "    info = 'off_100_256_400_only'#256_400\n",
    "    Model_info = '/' + Model_NAME + '_' + info + '/'\n",
    "    PARAMS_NAME = 'model_params.pt'\n",
    "    LOG_NAME = 'model_log'\n",
    "    try:\n",
    "      os.mkdir(os.path.join(OUTPUT_PATH + Model_info))\n",
    "    except OSError as error:\n",
    "      print(\"Folder doesn't exists\")\n",
    "      x = random.randint(1, 1000)\n",
    "      os.mkdir(os.path.join(OUTPUT_PATH + Model_info+str(x)))\n",
    "\n",
    "    PARAMS_PATH = os.path.join(OUTPUT_PATH + Model_info, PARAMS_NAME)\n",
    "    LOG_PATH = os.path.join(OUTPUT_PATH + Model_info, LOG_NAME)\n",
    "\n",
    "    setLogger(LOG_PATH, mode='w')\n",
    "\n",
    "    # Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    train_dataset = MyDataset(TRAIN_DATASET_DIR, 0, train_transform)\n",
    "    valid_dataset = MyDataset(TRAIN_DATASET_DIR, 1, eval_transform)\n",
    "    test_dataset = MyDataset(TRAIN_DATASET_DIR, 2, eval_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    model.apply(initWeights)\n",
    "\n",
    "    params = model.parameters()\n",
    "\n",
    "    params_wd, params_rest = [], []\n",
    "    for param_item in params:\n",
    "        if param_item.requires_grad:\n",
    "            (params_wd if param_item.dim() != 1 else params_rest).append(param_item)\n",
    "\n",
    "    param_groups = [{'params': params_wd, 'weight_decay': WEIGHT_DECAY},\n",
    "                    {'params': params_rest}]\n",
    "\n",
    "    optimizer = optim.SGD(param_groups, lr=LR, momentum=0.9)\n",
    "\n",
    "    if statePath:\n",
    "        logging.info('-' * 8)\n",
    "        logging.info('Load state_dict in {}'.format(statePath))\n",
    "        logging.info('-' * 8)\n",
    "\n",
    "        all_state = torch.load(statePath)\n",
    "\n",
    "        original_state = all_state['original_state']\n",
    "        optimizer_state = all_state['optimizer_state']\n",
    "        epoch = all_state['epoch']\n",
    "\n",
    "        model.load_state_dict(original_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "        startEpoch = epoch + 1\n",
    "\n",
    "    else:\n",
    "        startEpoch = 1\n",
    "\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=DECAY_EPOCH, gamma=0.2)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(startEpoch, EPOCHS + 1):\n",
    "        scheduler.step()\n",
    "\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "        if epoch % EVAL_PRINT_FREQUENCY == 0:\n",
    "            #adjust_bn_stats(model, device, train_loader)\n",
    "            best_acc = evaluate(model, device, valid_loader, epoch, optimizer, best_acc, PARAMS_PATH)\n",
    "\n",
    "    logging.info('\\nTest set accuracy: \\n')\n",
    "\n",
    "    # Load best network parmater to test\n",
    "    all_state = torch.load(PARAMS_PATH)\n",
    "    original_state = all_state['original_state']\n",
    "    optimizer_state = all_state['optimizer_state']\n",
    "    model.load_state_dict(original_state)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    #adjust_bn_stats(model, device, train_loader)\n",
    "    evaluate(model, device, test_loader, epoch, optimizer, best_acc, PARAMS_PATH)\n",
    "\n",
    "\n",
    "def myParseArgs():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    '-TRAIN_DIR' ,\n",
    "    '--TRAIN_DIR',\n",
    "    help='The path to load train_dataset',\n",
    "    type=str,\n",
    "    required=True\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    '-VALID_DIR',\n",
    "    '--VALID_DIR',\n",
    "    help='The path to load valid_dataset',\n",
    "    type=str,\n",
    "    required=True\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    '-TEST_DIR',\n",
    "    '--TEST_DIR',\n",
    "    help='The path to load test_dataset',\n",
    "    type=str,\n",
    "    required=True\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    '-g',\n",
    "    '--gpuNum',\n",
    "    help='Determine which gpu to use',\n",
    "    type=str,\n",
    "    choices=['0', '1', '2', '3'],\n",
    "    required=True\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    '-l',\n",
    "    '--statePath',\n",
    "    help='Path for loading model state',\n",
    "    type=str,\n",
    "    default=''\n",
    "  )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "\n",
    "  return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bf9394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -TRAIN_DIR TRAIN_DIR -VALID_DIR VALID_DIR -TEST_DIR TEST_DIR -g {0,1,2,3}\n",
      "                             [-l STATEPATH]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -TRAIN_DIR/--TRAIN_DIR, -VALID_DIR/--VALID_DIR, -TEST_DIR/--TEST_DIR, -g/--gpuNum\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-PC\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  args = myParseArgs()\n",
    "\n",
    "  os.environ['CUDA_VISIBLE_DEVICES'] = args.gpuNum\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786083cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running main function with arguments: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define your arguments or constants\n",
    "gpu_num = \"0\"  # Replace with your desired GPU number\n",
    "\n",
    "# Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_num\n",
    "\n",
    "# Define the main function (replace this with the actual implementation of your 'main' function)\n",
    "def main_function(args):\n",
    "    print(\"Running main function with arguments:\", args)\n",
    "\n",
    "# Call the main function with the arguments\n",
    "main_function(gpu_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005ac6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
