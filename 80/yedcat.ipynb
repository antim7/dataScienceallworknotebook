{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d519be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daae9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 SRM filtes\n",
    "from srm_filter_kernel import all_normalized_hpf_list\n",
    "# Global covariance pooling\n",
    "from MPNCOV import *  # MPNCOV\n",
    "#cover_dir = '/home/ahmed/Documents/suniward0.4/base/TRN/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab26b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32 // 2\n",
    "num_levels = 3\n",
    "EPOCHS = 400\n",
    "LR = 0.005\n",
    "\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "TRAIN_PRINT_FREQUENCY = 100\n",
    "EVAL_PRINT_FREQUENCY = 1\n",
    "DECAY_EPOCH = [50, 150, 250]\n",
    "\n",
    "# OUTPUT_PATH = Path(__file__).stem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddafcdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncation operation\n",
    "class TLU(nn.Module):\n",
    "    def __init__(self, threshold):\n",
    "        super(TLU, self).__init__()\n",
    "\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = torch.clamp(input, min=-self.threshold, max=self.threshold)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c6bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/erogol/a324cc054a3cdc30e278461da9f1a05e\n",
    "class SPPLayer(nn.Module):\n",
    "    def __init__(self, num_levels):\n",
    "        super(SPPLayer, self).__init__()\n",
    "\n",
    "        self.num_levels = num_levels\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, h, w = x.size()\n",
    "        pooling_layers = []\n",
    "        for i in range(self.num_levels):\n",
    "            kernel_size = h // (2 ** i)\n",
    "\n",
    "            tensor = F.avg_pool2d(x, kernel_size=kernel_size,\n",
    "                                  stride=kernel_size).view(bs, -1)\n",
    "            pooling_layers.append(tensor)\n",
    "        x = torch.cat(pooling_layers, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb97834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absult value operation\n",
    "class ABS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ABS, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = torch.abs(input)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0861f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add operation\n",
    "class ADD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ADD, self).__init__()\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output = torch.add(input1, input2)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364d3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing Module\n",
    "class HPF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HPF, self).__init__()\n",
    "\n",
    "        # Load 30 SRM Filters\n",
    "        all_hpf_list_5x5 = []\n",
    "\n",
    "        for hpf_item in all_normalized_hpf_list:\n",
    "            if hpf_item.shape[0] == 3:\n",
    "                hpf_item = np.pad(hpf_item, pad_width=((1, 1), (1, 1)), mode='constant')\n",
    "\n",
    "            all_hpf_list_5x5.append(hpf_item)\n",
    "\n",
    "        hpf_weight = nn.Parameter(torch.Tensor(all_hpf_list_5x5).view(30, 1, 5, 5), requires_grad=False)\n",
    "\n",
    "        self.hpf = nn.Conv2d(1, 30, kernel_size=5, padding=2, bias=False)\n",
    "        self.hpf.weight = hpf_weight\n",
    "\n",
    "        # Truncation, threshold = 3\n",
    "        self.tlu = TLU(3.0)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.hpf(input)\n",
    "        output = self.tlu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "'''\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    self.group1 = HPF()\n",
    "    self.conv2_1 = torch.nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, stride=1, groups=30, padding=1) # Sepconv Block 1 Layer 2\n",
    "    self.abs = ABS\n",
    "    self.conv2_2 = torch.nn.Conv2d(in_channels=60, out_channels=30, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn2 = nn.BatchNorm2d(30)\n",
    "\n",
    "    self.conv3_1 = torch.nn.Conv2d(in_channels=30, out_channels=60, kernel_size=3, stride=1, groups=30, padding=1) # Sepconv Block 2 Layer 3\n",
    "\n",
    "    self.conv3_2 = torch.nn.Conv2d(in_channels=60, out_channels=30, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn3 = nn.BatchNorm2d(30)\n",
    "\n",
    "    self.add = ADD\n",
    "\n",
    "    self.group2 = nn.Sequential(\n",
    "      nn.Conv2d(30, 32, kernel_size=3, padding=1),  #layer4\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\t  nn.AvgPool2d(kernel_size=5, padding=2, stride=2),\n",
    "\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1), #layer5\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "\t  nn.AvgPool2d(kernel_size=5, padding=2, stride=2),\n",
    "\n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1), #layer6\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "\t  nn.AvgPool2d(kernel_size=5, padding=2, stride=2),\n",
    "\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),#layer7\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "\tself.spp = SPPLayer(num_levels)\n",
    "    self.fc1 = torch.nn.Linear(2688,1024)\n",
    "\tself.fc2 = torch.nn.Linear(1024,2)\n",
    "\n",
    "\n",
    "  def forward(self, input):\n",
    "\toutput = input\n",
    "\n",
    "\tres = self.group1(output)\n",
    "\toutput = self.abs((self.conv2_1(res)))\n",
    "\toutput = F.relu(self.bn2(self.conv2_2(output)))\n",
    "\toutput = self.conv3_1(output)\n",
    "\toutput = F.relu(self.bn3(self.conv3_2(output)))\n",
    "\toutput = self.add(res ,output)\n",
    "\toutput = self.group2(output)\n",
    "\toutput = self.spp(output)\n",
    "\toutput = F.relu(self.fc1(output))\n",
    "\toutput = self.fc2(output)\n",
    "\treturn output\n",
    "'''\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.group1 = HPF()  # pre-processing Layer 1\n",
    "        # self.conv0.weight = torch.nn.Parameter(srm)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=30, out_channels=30, kernel_size=5, stride=1,\n",
    "                                     padding=2)  # Sepconv Block 1 Layer 2\n",
    "        self.abs = ABS()\n",
    "        self.bn1 = nn.BatchNorm2d(30)\n",
    "        # Trunc T= 3\n",
    "        self.tlu3 = TLU(3.0)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=30, out_channels=30, kernel_size=5, stride=1,\n",
    "                                     padding=2)  # Sepconv Block 2 Layer 3\n",
    "        self.bn2 = nn.BatchNorm2d(30)\n",
    "        # Trunc T = 1\n",
    "        self.tlu1 = TLU(1.0)\n",
    "        self.pool = torch.nn.AvgPool2d(kernel_size=5, stride=2,\n",
    "                                       padding=2)  # the same pool layer well be used to L3 and L4\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=30, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv4 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)  # Layer 4\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv5 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)  # Layer 5\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv6 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)  # Layer 6\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv7 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)  # Layer 7\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # self.spp_layer = SPPLayer(spp_level) # spp_level = 1 Global averge pooling\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(128, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 1024)\n",
    "        self.fc3 = torch.nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.group1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.abs(x)\n",
    "        # x =  F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.tlu3(x)\n",
    "\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = self.tlu1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        # print(\"conv3_2\",x.shape)\n",
    "        # x = torch.add(res ,x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        # x = self.spp_layer(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        # print(\"x\",x.shape)\n",
    "        # print(\"x\",x.view(-1, 256).shape)\n",
    "        x = x.view(-1, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return (x)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, sample in enumerate(train_loader):\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        data, label = sample['data'], sample['label']\n",
    "\n",
    "        shape = list(data.size())\n",
    "        data = data.reshape(shape[0] * shape[1], *shape[2:])\n",
    "        label = label.reshape(-1)\n",
    "\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        output = model(data)  # FP\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        losses.update(loss.item(), data.size(0))\n",
    "\n",
    "        loss.backward()  # BP\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)  # BATCH TIME = BATCH BP+FP\n",
    "        end = time.time()\n",
    "\n",
    "        if i % TRAIN_PRINT_FREQUENCY == 0:\n",
    "            logging.info('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                         'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                         'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                         'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses))\n",
    "\n",
    "\n",
    "# Adjust BN estimated value\n",
    "def adjust_bn_stats(model, device, train_loader):\n",
    "    model.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in train_loader:\n",
    "            data, label = sample['data'], sample['label']\n",
    "\n",
    "            shape = list(data.size())\n",
    "            data = data.reshape(shape[0] * shape[1], *shape[2:])\n",
    "            label = label.reshape(-1)\n",
    "\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "\n",
    "def evaluate(model, device, eval_loader, epoch, optimizer, best_acc, PARAMS_PATH):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in eval_loader:\n",
    "            data, label = sample['data'], sample['label']\n",
    "\n",
    "            shape = list(data.size())\n",
    "            data = data.reshape(shape[0] * shape[1], *shape[2:])\n",
    "            label = label.reshape(-1)\n",
    "\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = correct / (len(eval_loader.dataset) * 2)\n",
    "\n",
    "    if accuracy > best_acc and epoch > 180:\n",
    "        best_acc = accuracy\n",
    "        all_state = {\n",
    "            'original_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        torch.save(all_state, PARAMS_PATH)\n",
    "\n",
    "    logging.info('-' * 8)\n",
    "    logging.info('Eval accuracy: {:.4f}'.format(accuracy))\n",
    "    logging.info('Best accuracy:{:.4f}'.format(best_acc))\n",
    "    logging.info('-' * 8)\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "# Initialization\n",
    "def initWeights(module):\n",
    "    if type(module) == nn.Conv2d:\n",
    "        if module.weight.requires_grad:\n",
    "            nn.init.kaiming_normal_(module.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight.data, mean=0, std=0.01)\n",
    "        nn.init.constant_(module.bias.data, val=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9322dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "class AugData():\n",
    "    def __call__(self, sample):\n",
    "        data, label = sample['data'], sample['label']\n",
    "\n",
    "        # Rotation\n",
    "        rot = random.randint(0, 3)\n",
    "        data = np.rot90(data, rot, axes=[1, 2]).copy()\n",
    "\n",
    "        # Mirroring\n",
    "        if random.random() < 0.5:\n",
    "            data = np.flip(data, axis=2).copy()\n",
    "\n",
    "        new_sample = {'data': data, 'label': label}\n",
    "\n",
    "        return new_sample\n",
    "\n",
    "\n",
    "class ToTensor():\n",
    "    def __call__(self, sample):\n",
    "        data, label = sample['data'], sample['label']\n",
    "\n",
    "        data = np.expand_dims(data, axis=1)\n",
    "        data = data.astype(np.float32)\n",
    "        # data = data / 255.0\n",
    "\n",
    "        new_sample = {\n",
    "            'data': torch.from_numpy(data),\n",
    "            'label': torch.from_numpy(label).long(),\n",
    "        }\n",
    "\n",
    "        return new_sample\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, DATASET_DIR, partition, transform=None):\n",
    "        random.seed(1234)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        self.cover_dir = DATASET_DIR + '/cover'\n",
    "        self.stego_dir = DATASET_DIR + '/stego/' + Model_NAME\n",
    "\n",
    "\n",
    "        self.cover_dir_2 = '/media/castor/Elements/BOSS_off/BOSS_off_100' + '/cover'\n",
    "        self.stego_dir_2 = '/media/castor/Elements/BOSS_off/BOSS_off_100' + '/stego/' + Model_NAME\n",
    "        self.cover_dir_3 = '/media/castor/Elements/BOSS_off/BOSS_off_256' + '/cover'\n",
    "        self.stego_dir_3 = '/media/castor/Elements/BOSS_off/BOSS_off_256' + '/stego/' + Model_NAME\n",
    "        self.cover_dir_4 = '/media/castor/Elements/BOSS_off/BOSS_off_400' + '/cover'\n",
    "        self.stego_dir_4 = '/media/castor/Elements/BOSS_off/BOSS_off_400' + '/stego/' + Model_NAME\n",
    "\n",
    "        self.covers_list_all = [x.split('/')[-1] for x in glob(self.cover_dir + '/*')]\n",
    "        random.shuffle(self.covers_list_all)\n",
    "        if (partition == 0):\n",
    "            self.cover_list = self.covers_list_all[:4000]\n",
    "            self.cover_paths= [os.path.join(self.cover_dir, x) for x in  self.cover_list]\n",
    "            self.cover_paths_2 = [os.path.join(self.cover_dir_2, x) for x in self.cover_list]\n",
    "            self.cover_paths_3 = [os.path.join(self.cover_dir_3, x) for x in self.cover_list]\n",
    "            self.cover_paths_4 = [os.path.join(self.cover_dir_4, x) for x in self.cover_list]\n",
    "            self.cover_paths = self.cover_paths + self.cover_paths_2 + self.cover_paths_3 + self.cover_paths_4\n",
    "            #print (self.cover_paths_3)\n",
    "            #print self.cover_paths\n",
    "            self.stego_paths = [os.path.join(self.stego_dir, x) for x in self.cover_list]\n",
    "            self.stego_paths_2 = [os.path.join(self.stego_dir_2, x) for x in self.cover_list]\n",
    "            self.stego_paths_3 = [os.path.join(self.stego_dir_3, x) for x in self.cover_list]\n",
    "            self.stego_paths_4 = [os.path.join(self.stego_dir_4, x) for x in self.cover_list]\n",
    "            self.stego_paths = self.stego_paths + self.stego_paths_2 + self.stego_paths_3 + self.stego_paths_4\n",
    "\n",
    "            self.cover_steg = list(zip(self.cover_paths, self.stego_paths))\n",
    "            random.shuffle(self.cover_steg)\n",
    "            self.cover_paths, self.stego_paths = zip(*self.cover_steg)\n",
    "\n",
    "\n",
    "        if (partition == 1):\n",
    "            self.cover_list = self.covers_list_all[4000:5000]\n",
    "            self.cover_paths = [os.path.join(self.cover_dir, x) for x in self.cover_list]\n",
    "            self.stego_paths = [os.path.join(self.stego_dir, x) for x in self.cover_list]\n",
    "        if (partition == 2):\n",
    "            self.cover_list = self.covers_list_all[5000:10000]\n",
    "            self.cover_paths = [os.path.join(self.cover_dir, x) for x in self.cover_list]\n",
    "            self.stego_paths = [os.path.join(self.stego_dir, x) for x in self.cover_list]\n",
    "\n",
    "        assert len(self.cover_paths) != 0, \"cover_dir is empty\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cover_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_index = int(idx)\n",
    "\n",
    "        cover_path = self.cover_paths[file_index]\n",
    "        stego_path = self.stego_paths[file_index]\n",
    "        cover_data = cv2.imread(cover_path, -1)\n",
    "        stego_data = cv2.imread(stego_path, -1)\n",
    "        data = np.stack([cover_data, stego_data])\n",
    "        label = np.array([0, 1], dtype='int32')\n",
    "\n",
    "        sample = {'data': data, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def setLogger(log_path, mode='a'):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        # Logging to a file\n",
    "        file_handler = logging.FileHandler(log_path, mode=mode)\n",
    "        file_handler.setFormatter(logging.Formatter('%(asctime)s: %(message)s', '%Y-%m-%d %H:%M:%S'))\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    statePath = args.statePath\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        #AugData(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    eval_transform = transforms.Compose([\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "    TRAIN_DATASET_DIR = args.TRAIN_DIR\n",
    "    VALID_DATASET_DIR = args.VALID_DIR\n",
    "    TEST_DATASET_DIR = args.TEST_DIR\n",
    "\n",
    "    # Log files\n",
    "    global Model_NAME\n",
    "    Model_NAME = 'STEGO_Suniward_P0.2'\n",
    "    info = 'off_100_256_400'\n",
    "    Model_info = '/' + Model_NAME + '_' + info + '/'\n",
    "    PARAMS_NAME = 'model_params.pt'\n",
    "    LOG_NAME = 'model_log'\n",
    "    try:\n",
    "      os.mkdir(os.path.join(OUTPUT_PATH + Model_info))\n",
    "    except OSError as error:\n",
    "      print(\"Folder doesn't exists\")\n",
    "      x = random.randint(1, 1000)\n",
    "      os.mkdir(os.path.join(OUTPUT_PATH + Model_info+str(x)))\n",
    "\n",
    "    PARAMS_PATH = os.path.join(OUTPUT_PATH + Model_info, PARAMS_NAME)\n",
    "    LOG_PATH = os.path.join(OUTPUT_PATH + Model_info, LOG_NAME)\n",
    "\n",
    "    setLogger(LOG_PATH, mode='w')\n",
    "\n",
    "    # Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "    train_dataset = MyDataset(TRAIN_DATASET_DIR, 0, train_transform)\n",
    "    valid_dataset = MyDataset(TRAIN_DATASET_DIR, 1, eval_transform)\n",
    "    test_dataset = MyDataset(TRAIN_DATASET_DIR, 2, eval_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    model.apply(initWeights)\n",
    "\n",
    "    params = model.parameters()\n",
    "\n",
    "    params_wd, params_rest = [], []\n",
    "    for param_item in params:\n",
    "        if param_item.requires_grad:\n",
    "            (params_wd if param_item.dim() != 1 else params_rest).append(param_item)\n",
    "\n",
    "    param_groups = [{'params': params_wd, 'weight_decay': WEIGHT_DECAY},\n",
    "                    {'params': params_rest}]\n",
    "\n",
    "    optimizer = optim.SGD(param_groups, lr=LR, momentum=0.9)\n",
    "\n",
    "    if statePath:\n",
    "        logging.info('-' * 8)\n",
    "        logging.info('Load state_dict in {}'.format(statePath))\n",
    "        logging.info('-' * 8)\n",
    "\n",
    "        all_state = torch.load(statePath)\n",
    "\n",
    "        original_state = all_state['original_state']\n",
    "        optimizer_state = all_state['optimizer_state']\n",
    "        epoch = all_state['epoch']\n",
    "\n",
    "        model.load_state_dict(original_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "        startEpoch = epoch + 1\n",
    "\n",
    "    else:\n",
    "        startEpoch = 1\n",
    "\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=DECAY_EPOCH, gamma=0.2)\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(startEpoch, EPOCHS + 1):\n",
    "        scheduler.step()\n",
    "\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "        if epoch % EVAL_PRINT_FREQUENCY == 0:\n",
    "            adjust_bn_stats(model, device, train_loader)\n",
    "            best_acc = evaluate(model, device, valid_loader, epoch, optimizer, best_acc, PARAMS_PATH)\n",
    "\n",
    "    logging.info('\\nTest set accuracy: \\n')\n",
    "\n",
    "    # Load best network parmater to test\n",
    "    all_state = torch.load(PARAMS_PATH)\n",
    "    original_state = all_state['original_state']\n",
    "    optimizer_state = all_state['optimizer_state']\n",
    "    model.load_state_dict(original_state)\n",
    "    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    adjust_bn_stats(model, device, train_loader)\n",
    "    evaluate(model, device, test_loader, epoch, optimizer, best_acc, PARAMS_PATH)\n",
    "\n",
    "\n",
    "def myParseArgs(debug_bool):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '-TRAIN_DIR',\n",
    "        '--TRAIN_DIR',\n",
    "        help='The path to load train_dataset',\n",
    "        type=str,\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '-VALID_DIR',\n",
    "        '--VALID_DIR',\n",
    "        help='The path to load valid_dataset',\n",
    "        type=str,\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '-TEST_DIR',\n",
    "        '--TEST_DIR',\n",
    "        help='The path to load test_dataset',\n",
    "        type=str,\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '-g',\n",
    "        '--gpuNum',\n",
    "        help='Determine which gpu to use',\n",
    "        type=str,\n",
    "        choices=['0', '1', '2', '3'],\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '-l',\n",
    "        '--statePath',\n",
    "        help='Path for loading model state',\n",
    "        type=str,\n",
    "        default=''\n",
    "    )\n",
    "    if debug_bool:\n",
    "        args=parser;\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62636ca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m#args.test_dir = tst_base_name\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#args.model_name = 'JUNIWARD_P01\u001b[39;00m\n\u001b[0;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgpuNum\n\u001b[1;32m---> 16\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 150\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    148\u001b[0m LOG_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_log\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   os\u001b[38;5;241m.\u001b[39mmkdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mOUTPUT_PATH\u001b[49m \u001b[38;5;241m+\u001b[39m Model_info))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    152\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFolder doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OUTPUT_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #args = myParseArgs()\n",
    "    debug_bool = True\n",
    "    TRAIN_DIR = '/media/mehdi/castorx/elements/BOSS'\n",
    "    #tst_base_name = '/data/LIRMM/Steganalysis_project/HD_Bases/TST_100k_ASeed123'\n",
    "    args = myParseArgs(debug_bool=debug_bool)\n",
    "    if debug_bool:\n",
    "        args.statePath = None\n",
    "        args.gpuNum = '0'  # The reference number of the GPU in the system\n",
    "        args.TRAIN_DIR = TRAIN_DIR\n",
    "        args.VALID_DIR = TRAIN_DIR\n",
    "        args.TEST_DIR = TRAIN_DIR\n",
    "        #args.test_dir = tst_base_name\n",
    "        #args.model_name = 'JUNIWARD_P01\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpuNum\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb28f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
